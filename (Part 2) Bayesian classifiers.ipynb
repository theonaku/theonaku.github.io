{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedb598d",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09418b1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2aa1f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e1b37fc90a19a5feedba370ac4d63c",
     "grade": false,
     "grade_id": "cell-51e5da9ddd4cc99c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94413c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "355c9cc58d023424cd31e5aa8eddeb1e",
     "grade": false,
     "grade_id": "cell-a4d703cf4d20ee02",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Bayesian linear classifiers\n",
    "\n",
    "In this example you are going to fit a Bayesian logistic regression model, using two sets of features on data $\\mathbf{x} \\in \\mathbb{R}^2.\n",
    "\n",
    "The first one is a \"simple\" set of features, with just the two input features plus an intercept:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{\\text{simple}}(\\mathbf{x}) &= [ 1, x_1, x_2 ]\n",
    "\\end{align*}$$\n",
    "\n",
    "The second is up to you! For example, you could consider a set of quadratic polynomial features,\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\phi_{\\text{quadratic}}(\\mathbf{x}) &= [ 1, x_1, x_2, x_1x_2, x_1^2, x_2^2 ].\n",
    "\\end{align*}$$\n",
    "\n",
    "This example just includes quadratic terms, but you could also include higher order polynomial terms (e.g. $x_1^3, x_2^3$) or completely different features alltogether. It's up to you — but you should be prepared to justify your choice!\n",
    "\n",
    "You will also fit the model in two different ways:\n",
    "\n",
    "1. MAP estimation (penalized maximum likelihood)\n",
    "2. Laplace approximation (a Gaussian approximate posterior, centered at the mode)\n",
    "\n",
    "Here is a synthetic dataset that we'll be working with (plotting the training set only):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ecb2d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31183244bf0b5d69adc8b367d67c7472",
     "grade": false,
     "grade_id": "cell-07ecbcc2cafce227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = torch.load(\"data.pt\")\n",
    "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32ca843",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dbe785c96ee869ea03a5b51eacf4b3d9",
     "grade": false,
     "grade_id": "cell-791ab24aadbc08a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Here are definitions of two different feature maps, the \"simple\" one and the \"quadratic\" one.\n",
    "\n",
    "They define feature spaces in $\\mathbb{R}^3$ and $\\mathbb{R}^6$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f254ff7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a166ea18b83bb7485d4fc6453b80b721",
     "grade": false,
     "grade_id": "cell-318dc4fe1089d515",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features_simple(X):\n",
    "    return torch.concat((torch.ones_like(X[:,:1]), X), -1)\n",
    "\n",
    "def features_quadratic(X):\n",
    "    interactions = X.prod(-1, keepdim=True)\n",
    "    return torch.concat((torch.ones_like(X[:,:1]), \n",
    "                         X, X.pow(2), interactions), -1)\n",
    "\n",
    "print(\"Dimension of Phi, `features_simple`:\", features_simple(X_train).shape)\n",
    "print(\"Dimension of Phi, `features_quadratic`:\", features_quadratic(X_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d2e543",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "268279fdb7e070742cb5496dee1f8d7b",
     "grade": false,
     "grade_id": "cell-390950f2d4a30066",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #1 (3 points): Define the model\n",
    "\n",
    "The Bayesian logistic regression model we are working with has the form\n",
    "$$\\begin{align*}\n",
    "\\mathbf{w} &\\sim \\mathcal{N}(0, \\sigma^2 \\mathbf{I}) \\\\\n",
    "\\hat y_i &= \\mathrm{Logistic}(\\mathbf{w}^\\top \\phi(\\mathbf{x}_i)) \\\\\n",
    "y_i &\\sim \\mathrm{Bernoulli}(\\hat y_i)\n",
    "\\end{align*}$$\n",
    "where $i = 1,\\dots, N$ and the Logistic function is defined\n",
    "$$\\begin{align*}\n",
    "\\mathrm{Logistic}(z) &= \\frac{1}{1 + \\exp\\{-z\\}}.\n",
    "\\end{align*}$$\n",
    "It's implemented in pytorch as `torch.sigmoid`.\n",
    "\n",
    "The first step is to define two functions, one to make predictions given a weight vector $\\mathbf{w}$ and inputs $\\Phi$, and one which computes the log joint probability\n",
    "\n",
    "$$\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}, \\sigma^2).$$\n",
    "\n",
    "I've done the first one for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3477dc07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e5e5517d0441c8d5c12c738d656ab4a",
     "grade": false,
     "grade_id": "cell-2bfc837bc01b0b20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_probs_MAP(Phi, w):\n",
    "    \"\"\"\n",
    "    Given a \"design matrix\" Phi, and a point estimate w, compute p(y = 1 | Phi, w)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    w     : (D,) vector of weights\n",
    "\n",
    "    OUTPUT:\n",
    "    y_hat : (N,) vector of probabilities p(y=1 | Phi, w)\n",
    "    \"\"\"\n",
    "    return torch.sigmoid(Phi @ w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1fc311",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8254dfaca75f72cdb1b6644868222c7b",
     "grade": false,
     "grade_id": "B-log-joint",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def log_joint(Phi, y, w, sigma=10):\n",
    "    \"\"\"\n",
    "    Compute the joint probability of the data and the latent variables.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w     : (D,) vector of weights\n",
    "    sigma : scalar, standard deviation of Gaussian prior distribution p(w).\n",
    "            Leave this set to sigma=10 for purposes of this exercise\n",
    "\n",
    "    OUTPUT:\n",
    "    log_joint : the log probability log p(y, w | Phi, sigma), a torch scalar\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a307f74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7414775d4662069e3ed76e8c157c3663",
     "grade": true,
     "grade_id": "B-joint-test-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478df225",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "56476349ca26272aa489be1492bc6d3b",
     "grade": true,
     "grade_id": "B-joint-test-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc5535",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3462977d3068940b7acfa46272e66ea",
     "grade": false,
     "grade_id": "cell-91a0bc31c0fe7b9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK 2 (5 points): Implement MAP estimation\n",
    "\n",
    "Now you need to write a function which performs MAP estimation, i.e. penalized maximum likelihood estimation.\n",
    "\n",
    "This function should find the value $\\mathbf{w}_{MAP}$ that maximizes the log joint, i.e.\n",
    "\n",
    "$$\\mathbf{w}_{MAP} = \\mathrm{argmax}_{\\mathbf{w}}\\log p(\\mathbf{y}, \\mathbf{w} | \\mathbf{\\Phi}, \\sigma^2).$$\n",
    "\n",
    "To do this, you should **use pytorch autograd tools**. This will involve defining an initial value of the weights, computing a scalar loss function, and calling `.backward()`, and then performing gradient-based optimization. Take a look at the demo notebooks from previous lectures for examples…!\n",
    "\n",
    "* You **may feel free to use classes from `torch.optim`**. I would suggest the use of `torch.optim.SGD` or `torch.optim.Adagrad`.\n",
    "* Regardless of how you do this, you will need to decide on a stopping criteria for your optimization routine.\n",
    "* You will also need to decide on how to set the parameters (learning rate, momentum, anything else!) for your selected optimizer.\n",
    "\n",
    "Also, **your code should work for ANY features!**. We will test this out on not just the \"simple\" and \"quadratic\" features above, but also on your own custom choice of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ed4742",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0136517993392d94f124c530ccf7f33",
     "grade": false,
     "grade_id": "B-find-MAP",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_MAP(Phi, y):\n",
    "    \"\"\"\n",
    "    Find the MAP estimate of the log_joint method.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "\n",
    "\n",
    "    OUTPUT:\n",
    "    w      : (D,) vector of optimized weights\n",
    "    losses : list of losses at each iteration of the optimization algorithm.\n",
    "             Should be a list of scalars, which can be plotted afterward to\n",
    "             diagnose convergence.\n",
    "    \"\"\"\n",
    "\n",
    "    weights = torch.zeros(Phi.shape[1]).requires_grad_(True)\n",
    "    losses = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return weights.detach(), losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c9cf8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9db727380135210315c5bf49f0c0e9",
     "grade": false,
     "grade_id": "cell-0bb96b3160b8580d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following two cells call `find_MAP` to compute $\\mathbf{w}$ for both choices of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbd0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_simple, losses = find_MAP(features_simple(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afa5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_quad, losses = find_MAP(features_quadratic(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363aeaa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d4352ba194de7f7d86b9ccc051fc9e4",
     "grade": true,
     "grade_id": "B-test-MAP-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0927c5f2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb5505930918b72a9db67ba7e1f3f843",
     "grade": true,
     "grade_id": "B-test-MAP-2",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382a9a9f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0c088909096a75d2f55ad28f00f4b7db",
     "grade": false,
     "grade_id": "cell-f4589a3dbb0d0545",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Visualization: The following code visualizes the classifier result\n",
    "\n",
    "It plots the probability of being one class or the other using a color contour plot.\n",
    "\n",
    "The decision boundary is a dashed black line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boundary(X, y, pred):\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    Z = pred(torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=.8, levels=np.linspace(0, 1, 8))\n",
    "    \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n",
    "    \n",
    "    plt.contour(xx, yy, Z, levels=(0.5,), linestyles='dashed');\n",
    "    \n",
    "    # Plot the testing points\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_simple(X), w_MAP_simple))\n",
    "plt.title(\"MAP estimate, simple features\")\n",
    "train_accuracy = (predict_probs_MAP(features_simple(X_train), w_MAP_simple).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_simple(X_test), w_MAP_simple).round() == y_test).float().mean()\n",
    "print(\"Simple features: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boundary(X_train, y_train,\n",
    "              lambda X: predict_probs_MAP(features_quadratic(X), w_MAP_quad))\n",
    "plt.title(\"MAP estimate, quadratic features\")\n",
    "train_accuracy = (predict_probs_MAP(features_quadratic(X_train), w_MAP_quad).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_quadratic(X_test), w_MAP_quad).round() == y_test).float().mean()\n",
    "print(\"Polynomial features: training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51394eb5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd46820f17d02aa4b8c70889d43d93b7",
     "grade": false,
     "grade_id": "cell-3166631d224980bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #3 (5 points): Laplace approximation\n",
    "\n",
    "In the next section, you will fit an approximate posterior over the weights by using the Laplace approximation around the mode $\\mathbf{w}_{MAP}$ of the distribution you found above.\n",
    "\n",
    "This requires completing two functions:\n",
    "\n",
    "1. `compute_laplace_Cov` takes the data and the MAP estimate, and outputs a covariance matrix defined as the negative inverse Hessian of the log target density. (See the week 4 lecture slides for details on how to compute this!)\n",
    "2. `predict_bayes` makes predictions on new data points, by approximating $\\int p(y | x, w)p(w | \\mathcal{D})dw$ when using a Gaussian approximation to $p(w | \\mathcal{D})$. In the week 4 lecture slides we discussed three different ways of computing this — it is up to you to decide what method you would prefer, and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b04a17",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b04031d87b33a98b89706e5bee51f95",
     "grade": false,
     "grade_id": "B-laplace-cov",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_laplace_Cov(Phi, y, w_MAP, prior_std=10):\n",
    "    \"\"\"\n",
    "    Compute the Laplace approximation of the posterior covariance \n",
    "    in a logistic regression setting.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    sigma : scalar, standard deviation of Gaussian prior distribution p(w).\n",
    "            Leave this set to sigma=10 for purposes of this exercise\n",
    "\n",
    "    OUTPUT:\n",
    "    Cov : (D, D) posterior covariance matrix estimate defined by the Laplace \n",
    "          approximation\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da31180",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "532ca1f2a118acd58c737ec7545cd821",
     "grade": false,
     "grade_id": "B-laplace-predict",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict_bayes(Phi, w_MAP, Cov):\n",
    "    \"\"\"\n",
    "    Make predictions on new data points using an approximate posterior \n",
    "    w ~ MultivariateNormal(w_MAP, Cov)\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    Cov   : (D, D) approximate posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    y_hat : (N,) vector of probabilities p(y=1 | Phi)\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2c239",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b20317869f5427299c73a7aed504e984",
     "grade": false,
     "grade_id": "cell-e563aca374ddf46f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### The following cells call your functions above to compute the Laplace approximation and visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225e440e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13417e655e2239c0b67ca963c6d3d70a",
     "grade": false,
     "grade_id": "cell-9d607a490ff71c7f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Cov_simple = compute_laplace_Cov(features_simple(X_train), y_train, w_MAP_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Simple features: MAP\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_simple(X), w_MAP_simple))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Simple features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_simple(X), w_MAP_simple, Cov_simple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32adf170",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d6b7e143516c6b3c557c44f5aeb06d8",
     "grade": false,
     "grade_id": "cell-e6516c087b43b9a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "Cov_quad = compute_laplace_Cov(features_quadratic(X_train), y_train, w_MAP_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e09bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"Quadratic features: MAP\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_quadratic(X), w_MAP_quad))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"Quadratic features: Laplace\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_quadratic(X), w_MAP_quad, Cov_quad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9e528",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aebc89b362aa8eb7be81e982941cd56b",
     "grade": true,
     "grade_id": "B-test-Laplace-1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f87376",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94674dc5dd062cf200627a4c8cac432e",
     "grade": true,
     "grade_id": "B-test-Laplace-2",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5380",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "264d3021a237270917668394c5505e58",
     "grade": true,
     "grade_id": "B-test-Laplace-3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567389d5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e965d127f726030eaa6252f83ee6a35d",
     "grade": false,
     "grade_id": "cell-303212e96a305a6b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# TASK #4 (2 points): Model comparison\n",
    "\n",
    "You can compute the marginal likelihood approximation defined by the Laplace approximation.\n",
    "\n",
    "This estimate of the evidence can be used, even just looking at the training data, to help decide which of the two feature maps is more appropriate and better fits the data.\n",
    "\n",
    "This can help guard against potential overfitting if using features that are \"too complex\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dd545",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34fc8b4b795c39c977b6c55e09e2af59",
     "grade": false,
     "grade_id": "B-laplace-evidence",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_laplace_log_evidence(Phi, y, w_MAP, Cov):\n",
    "    \"\"\"\n",
    "    This computes the Laplace approximation to the marginal likelihood,\n",
    "    as defined in the Week 5 lectures.\n",
    "    \n",
    "    INPUT:\n",
    "    Phi   : (N, D) tensor of input features, where N is the number of \n",
    "            observations and D is the number of features\n",
    "    y     : (N,) vector of outputs (targets). Should be a `torch.FloatTensor`\n",
    "            containing zeros and ones\n",
    "    w_MAP : (D,) vector of optimized weights, at a mode of the target density\n",
    "    Cov   : (D, D) approximate posterior covariance matrix\n",
    "    \n",
    "    OUTPUT:\n",
    "    log_evidence : scalar value estimating `log p(y | Phi)`\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model evidence estimate (simple features):\",\n",
    "      compute_laplace_log_evidence(features_simple(X_train), y_train, w_MAP_simple, Cov_simple).item())\n",
    "\n",
    "print(\"Model evidence estimate (polynomial features):\",\n",
    "      compute_laplace_log_evidence(features_quadratic(X_train), y_train, w_MAP_quad, Cov_quad).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b63d8a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d355fa2808934801c1c2f480b80243d5",
     "grade": true,
     "grade_id": "B-test-evidence",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847aee57",
   "metadata": {},
   "source": [
    "# TASK #5 (2 points + 4 points): Define your own feature space\n",
    "\n",
    "Your next task is to define your own, custom feature space! This can be practically any deterministic function from $\\mathbb{R}^2$ to $\\mathbb{R}^D$, and you even get to pick the dimensionality $D$.\n",
    "\n",
    "In the following cells, you first define your feature space, and then we estimate the posterior using your code above. We report training accuracy, test accuracy, and model evidence, as well as plot the decision boundaries.\n",
    "\n",
    "Feel free to get quite creative here! However, you will be asked to defend your choice of feature space in the free-answer section at the bottom.\n",
    "\n",
    "**Make sure when you submit, you include whatever you consider the \"best\" possible choice!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73ef81",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "325d61748c236b889508938fd74f8cbb",
     "grade": false,
     "grade_id": "custom-features",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def features_student(X):\n",
    "    \"\"\"\n",
    "    Compute your own, custom set of features!\n",
    "    \n",
    "    INPUT:\n",
    "    X      : (N, 2) tensor of raw input data\n",
    "    \n",
    "    OUTPUT:\n",
    "    Phi    : (N, D) tensor of transformed inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    # return ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef001991",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_MAP_student, losses = find_MAP(features_student(X_train), y_train)\n",
    "plt.plot(losses);\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c85c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = (predict_probs_MAP(features_student(X_train), w_MAP_student).round() == y_train).float().mean()\n",
    "test_accuracy = (predict_probs_MAP(features_student(X_test), w_MAP_student).round() == y_test).float().mean()\n",
    "print(\"YOUR features! Training accuracy = %0.2f, test accuracy = %0.2f\" % (train_accuracy, test_accuracy))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234be4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov_student = compute_laplace_Cov(features_student(X_train), y_train, w_MAP_student)\n",
    "\n",
    "print(\"YOUR features! Model evidence estimate:\",\n",
    "      compute_laplace_log_evidence(features_student(X_train), y_train, w_MAP_student, Cov_student).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c257ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(\"YOUR features! MAP estimate\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_probs_MAP(features_student(X), w_MAP_student))\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title(\"YOUR features! Laplace estimate\")\n",
    "plot_boundary(X_train, y_train, \n",
    "              lambda X: predict_bayes(features_student(X), w_MAP_student, Cov_student))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c02290",
   "metadata": {},
   "source": [
    "# Please answer the following short questions:\n",
    "\n",
    "1. *[Feature selection]* How did you choose your features? Name at least two ways of comparing whether one set of features is \"better\" than another. Would these methods generally yield the same \"ranking\" of different features? Why or why not?\n",
    "2. *[Overfitting]* Is \"overfitting\" possible in this setting? Did you experience it? If so, how would you detect it? Does using the Laplace approximation \"help\" with overfitting, or does it not make a difference? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff22617",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a4eb22f7e1e4cc163820cba9faf28491",
     "grade": true,
     "grade_id": "free-response-features",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294930f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b66d8de81acc81add0f4b1b00fcfbb9b",
     "grade": false,
     "grade_id": "cell-260c2d89b6bc0c40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Ignore the following cells. They are used by the grading system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f8d628",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9f5e8fe46afe84edee037f19adfd74ed",
     "grade": true,
     "grade_id": "B-features-student",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00f7f9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "541fbece4305982f65dad80d55797b7c",
     "grade": true,
     "grade_id": "B-features-actuals",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# (GRADING)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
